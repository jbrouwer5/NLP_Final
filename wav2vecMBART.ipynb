{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, MBart50Tokenizer, MBartForConditionalGeneration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the europarl dataset training split\n",
    "dataset = load_dataset(\"tj-solergibert/Europarl-ST\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['original_speech', 'original_language', 'audio_path', 'segment_start', 'segment_end', 'transcriptions'],\n",
      "    num_rows: 31777\n",
      "})\n",
      "Dataset({\n",
      "    features: ['original_speech', 'original_language', 'audio_path', 'segment_start', 'segment_end', 'transcriptions'],\n",
      "    num_rows: 20\n",
      "})\n",
      "{'original_speech': 'Mr President, I know that I will not be popular for making a long speech at this time, but my two fellow-rapporteurs, with whom I have worked very closely as a team, have made short statements so I want to keep the team spirit together.', 'original_language': 'en', 'audio_path': 'en/audios/en.20080924.23.3-123.m4a', 'segment_start': 0.0, 'segment_end': 14.470000267028809, 'transcriptions': {'de': 'Herr Präsident! Ich weiß, dass ich mir keine Freunde mache, wenn ich um diese Uhrzeit eine lange Rede halte, doch meine beiden Mitberichterstatter, mit denen ich sehr eng im Team zusammengearbeitet habe, haben kurze Stellungnahmen abgegeben, sodass ich den Teamgeist zusammenhalten möchte.', 'en': 'Mr President, I know that I will not be popular for making a long speech at this time, but my two fellow-rapporteurs, with whom I have worked very closely as a team, have made short statements so I want to keep the team spirit together.', 'es': 'Señor Presidente, sé que no me haré popular ofreciendo un discurso prolongado en este momento, pero los dos colegas ponentes con quienes he trabajado estrechamente en equipo, han hecho exposiciones breves, así que quiero conservar el espíritu de equipo.', 'fr': \"Monsieur le Président, je sais qu'à ce stade-ci, une longue intervention ne me rendra pas très populaire mais mes deux collègues rapporteurs avec qui j'ai travaillé en équipe et en liens étroits ayant été brefs, je voudrais maintenir l'esprit d'équipe.\", 'it': '. Signor Presidente, so che a questo punto un lungo intervento non risulterebbe gradito e dal momento che i due correlatori assieme ai quali ho lavorato a stretto contatto hanno fatto dichiarazioni brevi voglio conformarmi allo spirito della squadra.', 'nl': 'Mijnheer de Voorzitter, ik weet dat ik mij niet populair maak door nu een lange toespraak te houden, maar mijn twee mederapporteurs, met wie ik als team zeer nauw heb samengewerkt, hebben korte verklaringen afgelegd en ik wil de teamgeest graag hoog houden.', 'pl': 'Panie przewodniczący! Wiem, że wygłoszenie w tej chwili długiej przemowy nie przysporzy mi popularności, ale skoro moi dwaj koledzy-sprawozdawcy, z którymi bardzo blisko współpracowałem w zespole, mieli możność krótkiego wypowiedzenia się, chciałbym zachować ducha zespołowego.', 'pt': 'Senhor Presidente, sei que não serei muito popular se fizer um longo discurso a esta hora, e realmente os meus dois co-relatores, com quem trabalhei em equipa num espírito de verdadeira cooperação, foram breves nas suas intervenções e não quero, portanto, quebrar o espírito de equipa.', 'ro': None}}\n",
      "{'original_speech': 'I would just like to say that there are more amendments in my report because my committee has been more ambitious in the improvements it wanted to make to the Commission proposal.', 'original_language': 'en', 'audio_path': 'en/audios/en.20080924.23.3-123.m4a', 'segment_start': 14.890000343322754, 'segment_end': 24.579999923706055, 'transcriptions': {'de': 'Ich möchte nur anmerken, dass es in meinem Bericht mehr Änderungsanträge gibt, weil mein Ausschuss stärker auf die Verbesserungen bedacht war, die er an dem Vorschlag der Kommission vornehmen wollte.', 'en': 'I would just like to say that there are more amendments in my report because my committee has been more ambitious in the improvements it wanted to make to the Commission proposal.', 'es': 'Tan sólo me gustaría decir que hay más enmiendas en mi ponencia porque mi comité ha sido más ambicioso en las mejoras que deseaba sugerir a la propuesta de la Comisión.', 'fr': \"Je souhaite simplement préciser qu'il y a dans mon rapport plus d'amendements parce que ma commission a été plus ambitieuse dans sa volonté d'améliorer la proposition de la Commission.\", 'it': 'Desidero solamente sottolineare che gli ulteriori emendamenti nella relazione sono dovuti al fatto che la mia commissione ha voluto essere più ambiziosa e apportare migliorie alla proposta della Commissione.', 'nl': 'Ik wil alleen zeggen dat mijn verslag meer amendementen bevat omdat mijn commissie ambitieuzer is geweest bij de verbeteringen die zij wilde aanbrengen in het voorstel van de Commissie.', 'pl': 'Pragnę tylko powiedzieć, że w moim sprawozdaniu jest więcej poprawek, gdyż moja komisja ambitniej podeszła do zadania, jakim było poprawienie wniosku Komisji.', 'pt': 'Gostaria apenas de dizer que há mais alterações no meu relatório porque a minha comissão foi mais ambiciosa nas melhorias que pretende introduzir na proposta da Comissão.', 'ro': None}}\n",
      "{'original_speech': 'I ask for your support to ensure that we have confident, well-informed consumers for electronic communications, who are also secure and know that their personal data is protected.', 'original_language': 'en', 'audio_path': 'en/audios/en.20080924.23.3-123.m4a', 'segment_start': 25.149999618530273, 'segment_end': 37.7400016784668, 'transcriptions': {'de': 'Ich bitte Sie um Ihre Unterstützung, um sicherzustellen, dass die Verbraucher gut informiert sind und Vertrauen in die elektronische Kommunikation haben, dass sie auch sicher sind und wissen, dass ihre personenbezogenen Daten geschützt sind.', 'en': 'I ask for your support to ensure that we have confident, well-informed consumers for electronic communications, who are also secure and know that their personal data is protected.', 'es': 'Solicito su apoyo para asegurar que tenemos consumidores de comunicaciones eléctronicas confiados y bien informados, que también están seguros y saben que sus datos personales están protegidos.', 'fr': \"J'en appelle à votre soutien pour faire en sorte que les consommateurs soient confiants et bien informés face aux communications électroniques, pour qu'ils aient la sécurité et que leurs données personnelles soient protégées.\", 'it': 'Chiedo il vostro sostegno per far sì che i consumatori possano avere fiducia ed essere ben informati in materia di comunicazione elettronica, oltre ad essere consapevoli e sicuri che i loro dati personali saranno tutelati.', 'nl': 'Ik vraag u nu om uw steun, zodat we kunnen zorgen voor goed voorgelichte consumenten die vertrouwen hebben in elektronische communicatie en die bovendien goed zijn beveiligd en weten dat hun persoonsgegevens worden beschermd.', 'pl': 'Proszę o państwa wsparcie celem zapewnienia, byśmy mieli pewnych swych praw, dobrze poinformowanych konsumentów narzędzi łączności elektronicznej, bezpiecznych i świadomych ochrony i danych osobowych.', 'pt': 'Peço o vosso apoio para assegurar que teremos consumidores confiantes e bem informados no domínio das comunicações electrónicas, e também para que eles se sintam seguros e saibam que os seus dados pessoais estão protegidos.', 'ro': None}}\n",
      "{'original_speech': 'I would like to thank all my colleagues on the committee who worked with me to put together some really big compromise amendments which we will pass today.', 'original_language': 'en', 'audio_path': 'en/audios/en.20080924.23.3-123.m4a', 'segment_start': 38.290000915527344, 'segment_end': 46.97999954223633, 'transcriptions': {'de': 'Ich möchte allen meinen Kollegen im Ausschuss danken, die mit mir an der Abfassung einiger wirklich großer Kompromissänderungsanträge gearbeitet haben, über die wir heute abstimmen werden.', 'en': 'I would like to thank all my colleagues on the committee who worked with me to put together some really big compromise amendments which we will pass today.', 'es': 'Me gustaría dar las gracias a todos mis colegas que trabajaron conmigo en el comité para reunir importantes enmiendas de compromiso que aprobaremos hoy.', 'fr': \"Je voudrais remercier tous les collègues de la commission qui m'ont aidé à rédiger quelques amendements de compromis substantiels que nous adopterons aujourd'hui.\", 'it': 'Vorrei ringraziare tutti i colleghi della commissione che hanno collaborato alla stesura di alcuni emendamenti di ampio compromesso che approveremo oggi.', 'nl': \"Ik wil graag mijn dank betuigen aan alle collega 's in de commissie die met mij hebben gewerkt aan het opstellen van enkele, echt grote compromisamendementen. Wij gaan die vandaag aannemen.\", 'pl': 'Pragnę podziękować również wszystkim moim kolegom w komisji, którzy pracowali ze mną celem złożenia kilku naprawdę dużych poprawek kompromisowych, które dzisiaj przyjmiemy.', 'pt': 'Quero agradecer aos meus colegas na comissão o trabalho que desenvolvemos conjuntamente para chegar a alterações de compromisso verdadeiramente importantes que hoje iremos aprovar.', 'ro': None}}\n",
      "{'original_speech': 'I would like particularly to thank Alexander Alvaro and the Civil Liberties Committee, with whom we worked very closely, for their part on the E-Privacy Directive.', 'original_language': 'en', 'audio_path': 'en/audios/en.20080924.23.3-123.m4a', 'segment_start': 47.439998626708984, 'segment_end': 56.349998474121094, 'transcriptions': {'de': 'Ich möchte insbesondere Alexander Alvaro und dem Ausschuss für bürgerliche Freiheiten, mit dem wir sehr eng zusammengearbeitet haben, für seinen Beitrag zur Richtlinie über den Schutz der Privatsphäre in der elektronischen Kommunikation danken.', 'en': 'I would like particularly to thank Alexander Alvaro and the Civil Liberties Committee, with whom we worked very closely, for their part on the E-Privacy Directive.', 'es': 'En especial, desearia dar las gracias a Alexander Alvaro y al Comité de Libertades Civiles, con los que hemos trabajado estrechamente, por su contribución a la Directiva sobre la intimidad.', 'fr': 'Mes remerciements vont, tout particulièrement, à Alexander Alvaro et à la commission des libertés civiles - avec lesquels nous avons travaillé en étroite collaboration - pour leur rôle dans la Directive vie privée et communications électroniques.', 'it': 'Desidero inoltre ringraziare in modo particolare l ’ onorevole Alvaro e la commissione per le libertà civili, assieme ai quali abbiamo lavorato a stretto contatto, per il loro apporto alla direttiva sulla vita privata e le comunicazioni elettroniche.', 'nl': 'Ik wil met name Alexander Alvaro en de Commissie burgerlijke vrijheden, met wie wij zeer nauw hebben samengewerkt, bedanken voor hun bijdrage aan de e-privacy-richtlijn.', 'pl': 'W szczególności pragnę podziękować panu posłowi Alexandrowi Alvaro oraz Komisji Wolności Obywatelskich, Sprawiedliwości i Spraw Wewnętrznych, z którymi współpracowaliśmy bardzo ściśle, za ich wkład w dyrektywę w sprawie ochrony prywatności w sektorze łączności elektronicznej.', 'pt': 'Agradeço em particular ao senhor deputado Alexander Alvaro e à Comissão das Liberdades Cívicas, com quem trabalhámos de perto, o seu contributo no que se prende com a Directiva Privacidade e Comunicações Electrónicas.', 'ro': None}}\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset where the speech is english, there is a french transcription, and the audio file is present \n",
    "dataset = dataset.filter(lambda example: example['original_language'] == 'en' and 'fr' in example['transcriptions'].keys() and example[\"transcriptions\"]['fr'] is not None and os.path.exists(example[\"audio_path\"]))\n",
    "print(dataset)\n",
    "\n",
    "# Select a subset of the filtered dataset for training\n",
    "dataset = dataset.select(range(20))\n",
    "print(dataset)\n",
    "\n",
    "# Display some examples from the filtered dataset\n",
    "for example in dataset.select(range(5)):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded wav2vec2 processor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded wav2vec2 model\n",
      "Loaded mBART tokenizer\n",
      "Loaded mBART model\n"
     ]
    }
   ],
   "source": [
    "# Load models and processors/tokenizers for wav2vec and marian\n",
    "wav2vec2_processor_name = \"facebook/wav2vec2-base-960h\"\n",
    "wav2vec2_processor = Wav2Vec2Processor.from_pretrained(wav2vec2_processor_name)\n",
    "print(\"Loaded wav2vec2 processor\")\n",
    "\n",
    "# Load wav2vec2.0 model\n",
    "wav2vec2_model = Wav2Vec2ForCTC.from_pretrained(wav2vec2_processor_name)\n",
    "print(\"Loaded wav2vec2 model\")\n",
    "\n",
    "# Load mBART tokenizer\n",
    "mbart_tokenizer_name = \"facebook/mbart-large-50\"\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(mbart_tokenizer_name, src_lang=\"en_XX\", tgt_lang=\"fr_XX\")\n",
    "print(\"Loaded mBART tokenizer\")\n",
    "\n",
    "# Load mBART model\n",
    "mbart_model = MBartForConditionalGeneration.from_pretrained(mbart_tokenizer_name)\n",
    "print(\"Loaded mBART model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBartForConditionalGeneration(\n",
       "  (model): MBartModel(\n",
       "    (shared): Embedding(250054, 1024, padding_idx=1)\n",
       "    (encoder): MBartEncoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): MBartDecoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device config \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wav2vec2_model.to(device)\n",
    "mbart_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['original_speech', 'original_language', 'audio_path', 'segment_start', 'segment_end', 'transcriptions'],\n",
      "    num_rows: 20\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/20 [00:00<?, ? examples/s]/var/folders/ds/zb_p84s57dl5dvdn322z_sd80000gn/T/ipykernel_13774/1717146883.py:13: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(audio_path, sr=16000, offset=segment_start, duration=segment_end-segment_start)\n",
      "/usr/local/lib/python3.12/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Map:   0%|          | 0/20 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/transformers/feature_extraction_utils.py:93\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_features'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Apply the preprocessing function to the dataset\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_speech\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_language\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msegment_start\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msegment_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranscriptions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/datasets/arrow_dataset.py:3161\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3156\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3157\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3158\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3159\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3160\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3161\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3162\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3163\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/datasets/arrow_dataset.py:3522\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3520\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3522\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3523\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3524\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/datasets/arrow_dataset.py:3421\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3420\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3421\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3423\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3424\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3425\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[26], line 19\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     16\u001b[0m inputs \u001b[38;5;241m=\u001b[39m wav2vec2_processor(audio, sampling_rate\u001b[38;5;241m=\u001b[39msr, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# pad the input to be length 3000 for wav2vec \u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m input_features \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_features\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# if input_features.shape[-1] < 3000:\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#     padding = torch.zeros((input_features.shape[0], 3000 - input_features.shape[-1]))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#     input_features = torch.cat([input_features, padding], dim=-1)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m input_features\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/transformers/feature_extraction_utils.py:95\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import librosa \n",
    "\n",
    "audio_directory = \".\"\n",
    "\n",
    "# preprocesses the dataset for training \n",
    "def preprocess(batch):\n",
    "    # gets audio meta data\n",
    "    audio_path = batch[\"audio_path\"]\n",
    "    segment_start = batch[\"segment_start\"]\n",
    "    segment_end = batch[\"segment_end\"]\n",
    "    \n",
    "    # load the audio\n",
    "    audio, sr = librosa.load(audio_path, sr=16000, offset=segment_start, duration=segment_end-segment_start)\n",
    "    \n",
    "    # process the audio using wav2vec\n",
    "    inputs = wav2vec2_processor(audio, sampling_rate=sr, return_tensors=\"pt\")\n",
    "    \n",
    "    # pad the input to be length 3000 for wav2vec \n",
    "    input_features = inputs.input_features.squeeze(0)\n",
    "    # if input_features.shape[-1] < 3000:\n",
    "    #     padding = torch.zeros((input_features.shape[0], 3000 - input_features.shape[-1]))\n",
    "    #     input_features = torch.cat([input_features, padding], dim=-1)\n",
    "    \n",
    "    batch[\"input_features\"] = input_features\n",
    "    \n",
    "    # tokenize the transcription of the original speech for wav2vec labels \n",
    "    original_transcription = batch[\"original_speech\"]\n",
    "    tokenized_originals = wav2vec2_processor.tokenizer(\n",
    "        original_transcription,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\"\n",
    "    ).input_ids.squeeze(0)\n",
    "    \n",
    "    batch[\"english_text\"] = tokenized_originals\n",
    "    \n",
    "    french_transcription = batch[\"transcriptions\"][\"fr\"]\n",
    "    \n",
    "    # Tokenize the target French text translation for marian\n",
    "    tokenized_labels = tokenizer(\n",
    "        text_target=french_transcription,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\"\n",
    "    ).input_ids.squeeze(0)\n",
    "    \n",
    "    batch[\"labels\"] = tokenized_labels\n",
    "    \n",
    "    return batch\n",
    "print(dataset)\n",
    "# Apply the preprocessing function to the dataset\n",
    "dataset = dataset.map(preprocess, remove_columns=[\"audio_path\", \"original_speech\", \"original_language\", \"segment_start\", \"segment_end\", \"transcriptions\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate function for tensor and padding before dataloader\n",
    "def collate_fn(batch):\n",
    "    input_features = [item['input_features'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    english_text = [item['english_text'] for item in batch]\n",
    "    \n",
    "    # Convert all inputs to tensors if they are not already\n",
    "    input_features = [torch.tensor(f) if not isinstance(f, torch.Tensor) else f for f in input_features]\n",
    "    labels = [torch.tensor(l) if not isinstance(l, torch.Tensor) else l for l in labels]\n",
    "    english_text = [torch.tensor(l) if not isinstance(l, torch.Tensor) else l for l in english_text]\n",
    "    \n",
    "    \n",
    "    # pad sequences\n",
    "    input_features = pad_sequence(input_features, batch_first=True)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=mt_tokenizer.pad_token_id)\n",
    "    english_text = pad_sequence(english_text, batch_first=True)\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'input_features': input_features,\n",
    "        'labels': labels, \n",
    "        'english_text': english_text\n",
    "    }\n",
    "\n",
    "# create data loader using collate function\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "print(dataloader)\n",
    "# optimizers\n",
    "wav2vec_optimizer = AdamW(wav2vec2_model.parameters(), lr=5e-5)\n",
    "mt_optimizer = AdamW(mbart_model.parameters(), lr=5e-5)\n",
    "\n",
    "# training loop\n",
    "num_epochs = 3\n",
    "output_dir = \"./fine_tuned_models\"\n",
    "\n",
    "        \n",
    "for epoch in range(3):\n",
    "    wav2vec2_model.train()\n",
    "    mbart_model.train()\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    for batch in tqdm(dataloader):\n",
    "        \n",
    "    \n",
    "        input_features = (batch[\"input_features\"])\n",
    "        transcription = batch[\"english_text\"]\n",
    "        target_ids = batch[\"labels\"]\n",
    "    \n",
    "        # forward pass through wav2vec model with english transcription labels\n",
    "        wav2vec_outputs = wav2vec2_model(input_features, labels=transcription)\n",
    "        \n",
    "        predicted_ids = wav2vec_outputs.logits.argmax(dim=-1)\n",
    "        \n",
    "        # decode the predicted ids to text\n",
    "        predicted_texts = wav2vec2_processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "        print(predicted_texts)\n",
    "\n",
    "        # tokenize the decoded text for the translation model\n",
    "        translation_inputs = mbart_tokenizer(predicted_texts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "        # forward pass through Translation model with wav2vec's transcription as input\n",
    "        translation_outputs = mbart_model(input_ids=translation_inputs, labels=target_ids)\n",
    "\n",
    "        # combine the losses for end-to-end training\n",
    "        # combined_loss = wav2vec_outputs.loss + translation_outputs.loss\n",
    "        # ombined_loss.backward()\n",
    "       \n",
    "        wav2vec_outputs.loss.backward()\n",
    "        translation_outputs.loss.backward()\n",
    "        \n",
    "        wav2vec_optimizer.step()\n",
    "        mt_optimizer.step()\n",
    "        \n",
    "        wav2vec_optimizer.zero_grad()\n",
    "        mt_optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += wav2vec_outputs.loss.item() + translation_outputs.loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {avg_loss}\")\n",
    "\n",
    "    # save the models\n",
    "    wav2vec2_model.save_pretrained(f\"./fine_tuned_models/wav2vec_epoch_{epoch + 1}\")\n",
    "    wav2vec2_processor.save_pretrained(f\"./fine_tuned_models/wav2vec_epoch_{epoch + 1}\")\n",
    "    mbart_model.save_pretrained(f\"./fine_tuned_models/mt_epoch_{epoch + 1}\")\n",
    "    mbart_tokenizer.save_pretrained(f\"./fine_tuned_models/mt_epoch_{epoch + 1}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(\"tj-solergibert/Europarl-ST\", split=\"test\")\n",
    "test_dataset = test_dataset.filter(lambda example: example['original_language'] == 'en' and 'fr' in example['transcriptions'].keys() and example[\"transcriptions\"]['fr'] is not None)\n",
    "print(len(test_dataset))\n",
    "print(test_dataset)\n",
    "test_dataset = test_dataset.select(range(10))\n",
    "test_dataset = test_dataset.map(preprocess, remove_columns=[\"audio_path\", \"original_speech\", \"original_language\", \"segment_start\", \"segment_end\", \"transcriptions\"])\n",
    "\n",
    "# Create DataLoader with custom collate_fn\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Paths to the saved models\n",
    "wav2vec_model_path = \"./fine_tuned_models/wav2vec_epoch_3\"  # Update with your final epoch\n",
    "mt_model_path = \"./fine_tuned_models/mt_epoch_3\"  # Update with your final epoch\n",
    "\n",
    "# Load the models\n",
    "# wav2vec_processor = wav2vecProcessor.from_pretrained(wav2vec_model_path)\n",
    "# wav2vec_model = wav2vecForConditionalGeneration.from_pretrained(wav2vec_model_path)\n",
    "# mt_tokenizer = MarianTokenizer.from_pretrained(mt_model_path)\n",
    "# mt_model = MarianMTModel.from_pretrained(mt_model_path)\n",
    "\n",
    "wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "wav2vec_model =  Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "mt_tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-50\")\n",
    "mt_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wav2vec_model.to(device)\n",
    "mt_model.to(device)\n",
    "\n",
    "wav2vec_model.eval()\n",
    "mt_model.eval()\n",
    "    \n",
    "epoch_loss = 0.0\n",
    "for batch in tqdm(test_dataloader):\n",
    "    input_features = batch[\"input_features\"].to(device)\n",
    "\n",
    "    original_speech = wav2vec_processor.batch_decode(batch[\"english_text\"].to(device), skip_special_tokens=True)[0]\n",
    "    print(\"Original Speech\", original_speech)\n",
    "    # Generate transcription using wav2vec model\n",
    "    with torch.no_grad():\n",
    "        wav2vec_outputs = wav2vec_model.generate(\n",
    "            input_features,\n",
    "            num_beams=5, \n",
    "            repetition_penalty=1.2, \n",
    "            no_repeat_ngram_size=2, \n",
    "            temperature=0.7,  \n",
    "            top_k=50,  \n",
    "            top_p=0.95  \n",
    "        )\n",
    "        \n",
    "        transcription = wav2vec_processor.batch_decode(wav2vec_outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(\"Generated Transcription:\", transcription)\n",
    "\n",
    "    # Translate the transcription using the translation model\n",
    "    tokenized_transcription = mt_tokenizer(transcription, return_tensors=\"pt\", padding=\"longest\", truncation=True)\n",
    "    tokenized_transcription = tokenized_transcription.input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        translated_tokens = mt_model.generate(tokenized_transcription)\n",
    "        translation = mt_tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"Translation to French:\", translation)\n",
    "\n",
    "avg_loss = epoch_loss / len(dataloader)\n",
    "print(f\"Loss: {avg_loss}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BLEU score of the original model and the trained model \n",
    "import sacrebleu\n",
    "import jiwer\n",
    "\n",
    "bleu_scores = []\n",
    "wer_scores = []\n",
    "\n",
    "for batch in tqdm(test_dataloader):\n",
    "    input_features = batch[\"input_features\"].to(device)\n",
    "    target_ids = batch[\"labels\"].to(device)\n",
    "\n",
    "    original_speech = wav2vec_processor.batch_decode(batch[\"english_text\"].to(device), skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(\"Original Speech\", original_speech)\n",
    "    # Generate transcription using wav2vec model\n",
    "    with torch.no_grad():\n",
    "        wav2vec_outputs = wav2vec_model.generate(\n",
    "            input_features,\n",
    "            num_beams=5, \n",
    "            repetition_penalty=1.2, \n",
    "            no_repeat_ngram_size=2, \n",
    "            temperature=0.7,  \n",
    "            top_k=50,  \n",
    "            top_p=0.95  \n",
    "        )\n",
    "        \n",
    "        transcription = wav2vec_processor.batch_decode(wav2vec_outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    # print(\"Generated Transcription:\", transcription)\n",
    "\n",
    "    # Translate the transcription using the translation model\n",
    "    tokenized_transcription = mt_tokenizer(transcription, return_tensors=\"pt\", padding=\"longest\", truncation=True)\n",
    "    tokenized_transcription = tokenized_transcription.input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        translated_tokens = mt_model.generate(tokenized_transcription)\n",
    "        translation = mt_tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    # print(\"Translation to French:\", translation)\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    reference = mt_tokenizer.decode(target_ids[0], skip_special_tokens=True)\n",
    "    hypothesis = translation \n",
    "    bleu = sacrebleu.corpus_bleu([hypothesis], [[reference]])\n",
    "    bleu_score = bleu.score\n",
    "    bleu_scores.append(bleu_score)\n",
    "    wer = jiwer.wer(reference, hypothesis)\n",
    "    wer_scores.append(wer)\n",
    "\n",
    "    print(f\"Transcription: {transcription}\")\n",
    "    print(f\"Generated Translation: {translation}\")\n",
    "    print(f\"Reference Translation: {reference}\")\n",
    "    print(f\"BLEU Score: {bleu_score}\\n\")\n",
    "    print(f\"WER: {wer}\\n\")\n",
    "\n",
    "avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "avg_wer_score = sum(wer_scores) / len(wer_scores)\n",
    "\n",
    "print(f\"Average BLEU Score: {avg_bleu_score}\")\n",
    "print(f\"Average WER: {avg_wer_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
