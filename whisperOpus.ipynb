{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, MarianMTModel, MarianTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the europarl dataset training split\n",
    "dataset = load_dataset(\"tj-solergibert/Europarl-ST\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['original_speech', 'original_language', 'audio_path', 'segment_start', 'segment_end', 'transcriptions'],\n",
      "    num_rows: 31777\n",
      "})\n",
      "Dataset({\n",
      "    features: ['original_speech', 'original_language', 'audio_path', 'segment_start', 'segment_end', 'transcriptions'],\n",
      "    num_rows: 20\n",
      "})\n",
      "{'original_speech': 'Mr President, I know that I will not be popular for making a long speech at this time, but my two fellow-rapporteurs, with whom I have worked very closely as a team, have made short statements so I want to keep the team spirit together.', 'original_language': 'en', 'audio_path': 'en/audios/en.20080924.23.3-123.m4a', 'segment_start': 0.0, 'segment_end': 14.470000267028809, 'transcriptions': {'de': 'Herr Präsident! Ich weiß, dass ich mir keine Freunde mache, wenn ich um diese Uhrzeit eine lange Rede halte, doch meine beiden Mitberichterstatter, mit denen ich sehr eng im Team zusammengearbeitet habe, haben kurze Stellungnahmen abgegeben, sodass ich den Teamgeist zusammenhalten möchte.', 'en': 'Mr President, I know that I will not be popular for making a long speech at this time, but my two fellow-rapporteurs, with whom I have worked very closely as a team, have made short statements so I want to keep the team spirit together.', 'es': 'Señor Presidente, sé que no me haré popular ofreciendo un discurso prolongado en este momento, pero los dos colegas ponentes con quienes he trabajado estrechamente en equipo, han hecho exposiciones breves, así que quiero conservar el espíritu de equipo.', 'fr': \"Monsieur le Président, je sais qu'à ce stade-ci, une longue intervention ne me rendra pas très populaire mais mes deux collègues rapporteurs avec qui j'ai travaillé en équipe et en liens étroits ayant été brefs, je voudrais maintenir l'esprit d'équipe.\", 'it': '. Signor Presidente, so che a questo punto un lungo intervento non risulterebbe gradito e dal momento che i due correlatori assieme ai quali ho lavorato a stretto contatto hanno fatto dichiarazioni brevi voglio conformarmi allo spirito della squadra.', 'nl': 'Mijnheer de Voorzitter, ik weet dat ik mij niet populair maak door nu een lange toespraak te houden, maar mijn twee mederapporteurs, met wie ik als team zeer nauw heb samengewerkt, hebben korte verklaringen afgelegd en ik wil de teamgeest graag hoog houden.', 'pl': 'Panie przewodniczący! Wiem, że wygłoszenie w tej chwili długiej przemowy nie przysporzy mi popularności, ale skoro moi dwaj koledzy-sprawozdawcy, z którymi bardzo blisko współpracowałem w zespole, mieli możność krótkiego wypowiedzenia się, chciałbym zachować ducha zespołowego.', 'pt': 'Senhor Presidente, sei que não serei muito popular se fizer um longo discurso a esta hora, e realmente os meus dois co-relatores, com quem trabalhei em equipa num espírito de verdadeira cooperação, foram breves nas suas intervenções e não quero, portanto, quebrar o espírito de equipa.', 'ro': None}}\n",
      "{'original_speech': 'I would just like to say that there are more amendments in my report because my committee has been more ambitious in the improvements it wanted to make to the Commission proposal.', 'original_language': 'en', 'audio_path': 'en/audios/en.20080924.23.3-123.m4a', 'segment_start': 14.890000343322754, 'segment_end': 24.579999923706055, 'transcriptions': {'de': 'Ich möchte nur anmerken, dass es in meinem Bericht mehr Änderungsanträge gibt, weil mein Ausschuss stärker auf die Verbesserungen bedacht war, die er an dem Vorschlag der Kommission vornehmen wollte.', 'en': 'I would just like to say that there are more amendments in my report because my committee has been more ambitious in the improvements it wanted to make to the Commission proposal.', 'es': 'Tan sólo me gustaría decir que hay más enmiendas en mi ponencia porque mi comité ha sido más ambicioso en las mejoras que deseaba sugerir a la propuesta de la Comisión.', 'fr': \"Je souhaite simplement préciser qu'il y a dans mon rapport plus d'amendements parce que ma commission a été plus ambitieuse dans sa volonté d'améliorer la proposition de la Commission.\", 'it': 'Desidero solamente sottolineare che gli ulteriori emendamenti nella relazione sono dovuti al fatto che la mia commissione ha voluto essere più ambiziosa e apportare migliorie alla proposta della Commissione.', 'nl': 'Ik wil alleen zeggen dat mijn verslag meer amendementen bevat omdat mijn commissie ambitieuzer is geweest bij de verbeteringen die zij wilde aanbrengen in het voorstel van de Commissie.', 'pl': 'Pragnę tylko powiedzieć, że w moim sprawozdaniu jest więcej poprawek, gdyż moja komisja ambitniej podeszła do zadania, jakim było poprawienie wniosku Komisji.', 'pt': 'Gostaria apenas de dizer que há mais alterações no meu relatório porque a minha comissão foi mais ambiciosa nas melhorias que pretende introduzir na proposta da Comissão.', 'ro': None}}\n",
      "{'original_speech': 'I ask for your support to ensure that we have confident, well-informed consumers for electronic communications, who are also secure and know that their personal data is protected.', 'original_language': 'en', 'audio_path': 'en/audios/en.20080924.23.3-123.m4a', 'segment_start': 25.149999618530273, 'segment_end': 37.7400016784668, 'transcriptions': {'de': 'Ich bitte Sie um Ihre Unterstützung, um sicherzustellen, dass die Verbraucher gut informiert sind und Vertrauen in die elektronische Kommunikation haben, dass sie auch sicher sind und wissen, dass ihre personenbezogenen Daten geschützt sind.', 'en': 'I ask for your support to ensure that we have confident, well-informed consumers for electronic communications, who are also secure and know that their personal data is protected.', 'es': 'Solicito su apoyo para asegurar que tenemos consumidores de comunicaciones eléctronicas confiados y bien informados, que también están seguros y saben que sus datos personales están protegidos.', 'fr': \"J'en appelle à votre soutien pour faire en sorte que les consommateurs soient confiants et bien informés face aux communications électroniques, pour qu'ils aient la sécurité et que leurs données personnelles soient protégées.\", 'it': 'Chiedo il vostro sostegno per far sì che i consumatori possano avere fiducia ed essere ben informati in materia di comunicazione elettronica, oltre ad essere consapevoli e sicuri che i loro dati personali saranno tutelati.', 'nl': 'Ik vraag u nu om uw steun, zodat we kunnen zorgen voor goed voorgelichte consumenten die vertrouwen hebben in elektronische communicatie en die bovendien goed zijn beveiligd en weten dat hun persoonsgegevens worden beschermd.', 'pl': 'Proszę o państwa wsparcie celem zapewnienia, byśmy mieli pewnych swych praw, dobrze poinformowanych konsumentów narzędzi łączności elektronicznej, bezpiecznych i świadomych ochrony i danych osobowych.', 'pt': 'Peço o vosso apoio para assegurar que teremos consumidores confiantes e bem informados no domínio das comunicações electrónicas, e também para que eles se sintam seguros e saibam que os seus dados pessoais estão protegidos.', 'ro': None}}\n",
      "{'original_speech': 'I would like to thank all my colleagues on the committee who worked with me to put together some really big compromise amendments which we will pass today.', 'original_language': 'en', 'audio_path': 'en/audios/en.20080924.23.3-123.m4a', 'segment_start': 38.290000915527344, 'segment_end': 46.97999954223633, 'transcriptions': {'de': 'Ich möchte allen meinen Kollegen im Ausschuss danken, die mit mir an der Abfassung einiger wirklich großer Kompromissänderungsanträge gearbeitet haben, über die wir heute abstimmen werden.', 'en': 'I would like to thank all my colleagues on the committee who worked with me to put together some really big compromise amendments which we will pass today.', 'es': 'Me gustaría dar las gracias a todos mis colegas que trabajaron conmigo en el comité para reunir importantes enmiendas de compromiso que aprobaremos hoy.', 'fr': \"Je voudrais remercier tous les collègues de la commission qui m'ont aidé à rédiger quelques amendements de compromis substantiels que nous adopterons aujourd'hui.\", 'it': 'Vorrei ringraziare tutti i colleghi della commissione che hanno collaborato alla stesura di alcuni emendamenti di ampio compromesso che approveremo oggi.', 'nl': \"Ik wil graag mijn dank betuigen aan alle collega 's in de commissie die met mij hebben gewerkt aan het opstellen van enkele, echt grote compromisamendementen. Wij gaan die vandaag aannemen.\", 'pl': 'Pragnę podziękować również wszystkim moim kolegom w komisji, którzy pracowali ze mną celem złożenia kilku naprawdę dużych poprawek kompromisowych, które dzisiaj przyjmiemy.', 'pt': 'Quero agradecer aos meus colegas na comissão o trabalho que desenvolvemos conjuntamente para chegar a alterações de compromisso verdadeiramente importantes que hoje iremos aprovar.', 'ro': None}}\n",
      "{'original_speech': 'I would like particularly to thank Alexander Alvaro and the Civil Liberties Committee, with whom we worked very closely, for their part on the E-Privacy Directive.', 'original_language': 'en', 'audio_path': 'en/audios/en.20080924.23.3-123.m4a', 'segment_start': 47.439998626708984, 'segment_end': 56.349998474121094, 'transcriptions': {'de': 'Ich möchte insbesondere Alexander Alvaro und dem Ausschuss für bürgerliche Freiheiten, mit dem wir sehr eng zusammengearbeitet haben, für seinen Beitrag zur Richtlinie über den Schutz der Privatsphäre in der elektronischen Kommunikation danken.', 'en': 'I would like particularly to thank Alexander Alvaro and the Civil Liberties Committee, with whom we worked very closely, for their part on the E-Privacy Directive.', 'es': 'En especial, desearia dar las gracias a Alexander Alvaro y al Comité de Libertades Civiles, con los que hemos trabajado estrechamente, por su contribución a la Directiva sobre la intimidad.', 'fr': 'Mes remerciements vont, tout particulièrement, à Alexander Alvaro et à la commission des libertés civiles - avec lesquels nous avons travaillé en étroite collaboration - pour leur rôle dans la Directive vie privée et communications électroniques.', 'it': 'Desidero inoltre ringraziare in modo particolare l ’ onorevole Alvaro e la commissione per le libertà civili, assieme ai quali abbiamo lavorato a stretto contatto, per il loro apporto alla direttiva sulla vita privata e le comunicazioni elettroniche.', 'nl': 'Ik wil met name Alexander Alvaro en de Commissie burgerlijke vrijheden, met wie wij zeer nauw hebben samengewerkt, bedanken voor hun bijdrage aan de e-privacy-richtlijn.', 'pl': 'W szczególności pragnę podziękować panu posłowi Alexandrowi Alvaro oraz Komisji Wolności Obywatelskich, Sprawiedliwości i Spraw Wewnętrznych, z którymi współpracowaliśmy bardzo ściśle, za ich wkład w dyrektywę w sprawie ochrony prywatności w sektorze łączności elektronicznej.', 'pt': 'Agradeço em particular ao senhor deputado Alexander Alvaro e à Comissão das Liberdades Cívicas, com quem trabalhámos de perto, o seu contributo no que se prende com a Directiva Privacidade e Comunicações Electrónicas.', 'ro': None}}\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset where the speech is english, there is a french transcription, and the audio file is present \n",
    "dataset = dataset.filter(lambda example: example['original_language'] == 'en' and 'fr' in example['transcriptions'].keys() and example[\"transcriptions\"]['fr'] is not None and os.path.exists(example[\"audio_path\"]))\n",
    "print(dataset)\n",
    "\n",
    "# Select a subset of the filtered dataset for training\n",
    "dataset = dataset.select(range(20))\n",
    "print(dataset)\n",
    "\n",
    "# Display some examples from the filtered dataset\n",
    "for example in dataset.select(range(5)):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded whisper processor\n",
      "Loaded whisper conditional generator\n",
      "Loaded mt tokenizer\n",
      "Loaded mt model\n"
     ]
    }
   ],
   "source": [
    "# Load models and processors/tokenizers for whisper and marian\n",
    "whisper_model_name = \"openai/whisper-tiny\"\n",
    "translation_model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "\n",
    "whisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)\n",
    "print(\"Loaded whisper processor\")\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_name)\n",
    "print(\"Loaded whisper conditional generator\")\n",
    "mt_tokenizer = MarianTokenizer.from_pretrained(translation_model_name)\n",
    "print(\"Loaded mt tokenizer\")\n",
    "mt_model = MarianMTModel.from_pretrained(translation_model_name)\n",
    "print(\"Loaded mt model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(59514, 512, padding_idx=59513)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=59514, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device config \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "whisper_model.to(device)\n",
    "mt_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa \n",
    "\n",
    "audio_directory = \".\"\n",
    "\n",
    "# preprocesses the dataset for training \n",
    "def preprocess(batch):\n",
    "    # gets audio meta data\n",
    "    audio_path = batch[\"audio_path\"]\n",
    "    segment_start = batch[\"segment_start\"]\n",
    "    segment_end = batch[\"segment_end\"]\n",
    "    \n",
    "    # load the audio\n",
    "    audio, sr = librosa.load(audio_path, sr=16000, offset=segment_start, duration=segment_end-segment_start)\n",
    "    \n",
    "    # process the audio using whisper\n",
    "    inputs = whisper_processor(audio, sampling_rate=sr, return_tensors=\"pt\")\n",
    "    \n",
    "    # pad the input to be length 3000 for whisper \n",
    "    input_features = inputs.input_features.squeeze(0)\n",
    "    # if input_features.shape[-1] < 3000:\n",
    "    #     padding = torch.zeros((input_features.shape[0], 3000 - input_features.shape[-1]))\n",
    "    #     input_features = torch.cat([input_features, padding], dim=-1)\n",
    "    \n",
    "    batch[\"input_features\"] = input_features\n",
    "    \n",
    "    # tokenize the transcription of the original speech for whisper labels \n",
    "    original_transcription = batch[\"original_speech\"]\n",
    "    tokenized_originals = whisper_processor.tokenizer(\n",
    "        original_transcription,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\"\n",
    "    ).input_ids.squeeze(0)\n",
    "    \n",
    "    batch[\"english_text\"] = tokenized_originals\n",
    "    \n",
    "    french_transcription = batch[\"transcriptions\"][\"fr\"]\n",
    "    \n",
    "    # Tokenize the target French text translation for marian\n",
    "    tokenized_labels = mt_tokenizer(\n",
    "        text_target=french_transcription,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\"\n",
    "    ).input_ids.squeeze(0)\n",
    "    \n",
    "    batch[\"labels\"] = tokenized_labels\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "dataset = dataset.map(preprocess, remove_columns=[\"audio_path\", \"original_speech\", \"original_language\", \"segment_start\", \"segment_end\", \"transcriptions\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x13c3e5fa0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Presidentut,, you mentioned that word several times.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:02<00:56,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' President, let us hope that the American proposals for purchases of toxic assets do work, because, if they do not, the contagion will almost certainly spread over here.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:05<00:48,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' would like a from Mr.iuya and Mr Almunia that we already do have other defensesences in place.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:07<00:39,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' sure sure that, in due course, we will have to review our regulatory defensesences, but this can and shouldn not be done reciproitateately in in the heat of the crisis.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:09<00:38,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Monday, on Monday we were told by a McCreevy that hedge funds and private equity were not the cause of the turmoil turmoil.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:12<00:37,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' want like to the thank Alexander Alvaro and the Civil Liberties Committee with with the we worked very closely, for their part on the E-Privacy Directive.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:14<00:33,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' when the flames and embers are extinguished can we turn to post-mortems as to how this happened and what is needed to avoid it happening again.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:17<00:30,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' most vital thing which which we must bring about immediately is is the restoration of confidence.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:19<00:26,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' are moving from liquidity liquidity problem to sol solvency problem.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:20<00:22,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' just like want to say this you is more amendments on my reports, my committee has been more ambitious in the improvements it wanted to make to the Commission proposal.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:22<00:19,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' would urge that the leaders of the world,�s most important economies of America, Europe, the Middle and Far East meet together within days and assure the world unequivocally that whatever is necessary to douse the flames will be provided unequivocally wherever those flames may erupt.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:24<00:17,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" would like all of you to give us a huge majority for this, when when when we come to negotiate with Commissioner Commission and Council, we will do our very best for Europe's� Cons Cons.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [00:26<00:15,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' us see how we can set a it.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:28<00:13,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' is now up to the political leaders of the highest level, a summit indeed – of the free-- economies – come together – no excuses – no holding back – no arguments – – take full responsibility for restoring confidence.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:30<00:11,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' we have our lenders of last resort ready to meet the very worst, might occur in such a circumstance.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:31<00:09,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' would risk overreacting, imposing our, wrongly-, over-draconian conditions, would only harm the prospects for investment in our economies, our for the future.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:33<00:07,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' President, well know that I will not be popular for making a long speech at this time, but my two other-rapporteur, with whom I worked worked, closely, a team, have made short statements, I want to keep the team spirit together.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [00:35<00:05,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' would like thanked thank all my colleagues on the committee who worked with me to put together some really big compromise amendments which we will pass today.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:37<00:03,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ask for your support to ensure that we have competent well well-in- consumers, electronic communications and and also also secure and knowing that their personal data is protected.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [00:39<00:01,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['idence is the vital base on which a vibrant financial system and a vibrant global economy rests.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:41<00:00,  2.06s/it]\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.895303186774254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' would like all thank all my colleagues on the committee who worked with me to put together some really big compromise amendments which we will pass today.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:02<00:52,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' us see how we can set about it.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:04<00:39,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' most vital thing, which we must bring about immediately, is the restoration of confidence.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:06<00:33,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' we have our lenders of last resort ready to meet the very worst which might occur in such a circumstance?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:08<00:30,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a is now up to the political leaders of the highest level – a summit indeed of of the free-market economies to come together – no excuses, no holding back, no arguments to to take full responsibility for restoring confidence.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:09<00:28,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' would like to to thank Alexander Alvaro and the Civil Liberties Committee for whom whom we worked very closely, for their part on the E-Privacy Directive.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:11<00:25,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Almuniaia you mentioned the word several times.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:13<00:24,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a Alm, on the we were told by Commissioner Creevy that hedge funds and private equity were not the cause of the current term.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:16<00:24,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a would like assurance from Mr Jouyet and Mr Almunia that we really do have our defences in place.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:18<00:22,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a would like all of you to give us a huge majority for this so that, when we come to negotiate with the Commission and Council, we will do our very best for Europe consumers� s consumers.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:19<00:19,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a President, I know that I will not be popular for making a long speech at this time, but my two fellow-rapporteurs, with whom I have worked very closely as a team, have made short statements so I want to keep the team spirit together.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:22<00:18,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' aidence is the vital base on which a vibrant financial system and a vibrant global economy rests.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [00:23<00:15,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a would like like to say that there are more amendments for my report because my committee has been more ambitious in the improvements it wanted to make to the Commission proposal.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:25<00:13,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a is true that, in due course, we will have to review our regulatory defences, but this cannot and should not be done precipitately. in the heat of the crisis.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:27<00:11,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a would risk overreacting, imposing unnecessary, wrongly directed over over-recraconian conditions which would only harm the prospects for investment in our economies and jobs for the future.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:29<00:09,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a President, let us hope that the American proposals for purchases of toxic assets do work, because, if they do not, the contagion will almost certainly spread over here.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:31<00:07,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a would urge that the leaders of the world s� s most important economies of America, Europe, the Middle and Far East meet together within days and assure the world unequivocally that whatever is necessary to douse the flames will be provided unequivocally wherever those flames may erupt.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [00:34<00:06,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a when the flames and embers are extinguished can we turn to post-mortems as to how this happened and what is needed to avoid it happening again.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:36<00:04,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a ask for your support to ensure that we have confident, well-informed consumers for electronic communications who who are also secure and know that their personal data is protected.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [00:38<00:02,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' are moving from a a problem to a solvency problem.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:41<00:00,  2.07s/it]\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.3270757138729095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a President, let us hope that the American proposals for purchases of toxic assets do work, because, if they do not, the contagion will almost certainly spread over here.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:02<00:46,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" a would like all of you to give us a huge majority for this so that, when we come to negotiate with the Commission and Council, we will do our very best for Europe's� s consumers.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:04<00:39,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a would like assurance from Mr Jouyet and Mr Almunia that we really do have our defences in place.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:06<00:37,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' most vital thing, which we must bring about immediately, is the restoration of confidence.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:09<00:37,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a would just like to say that there are more amendments in my report because my committee has been more ambitious in the improvements it wanted to make to the Commission proposal.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:11<00:33,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Almunia, you mentioned the word several times.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:13<00:29,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a when the flames and embers are extinguished can we turn to post-mortems as to how this happened and what is needed to avoid it happening again.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:15<00:28,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a is true that, in due course, we will have to review our regulatory defences, but this cannot and should not be done precipitately, in the heat of the crisis.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:18<00:28,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a would like to thank all my colleagues on the committee who worked with me to put together some really big compromise amendments which we will pass today.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:20<00:24,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a are moving from a liquidity problem to a solvency problem to To']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:22<00:22,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['idence is the vital base on which a vibrant financial system and a vibrant global economy rests.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:24<00:20,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a President, I know that I will not be popular for making a long speech at this time, but my two fellow-rapporteurs, with whom I have worked very closely as a team, have made short statements so I want to keep the team spirit together.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [00:28<00:20,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a President, on Monday we were told by Commissioner McCreevy that hedge funds and private equity were not the cause of the current turmoil.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:30<00:17,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a would urge that the leaders of the world ’ s most important economies of America, Europe, the Middle and Far East meet together within days and assure the world unequivocally that whatever is necessary to douse the flames will be provided unequivocally wherever those flames may erupt.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:32<00:14,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' would like particularly to thank Alexander Alvaro and the Civil Liberties Committee, with whom we worked very closely, for their part on the E-Privacy Directive.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:34<00:11,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' we have our lenders of last resort ready to meet the very worst which might occur in such a circumstance?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:36<00:08,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' is now up to the political leaders of the highest level – a summit indeed – of the free-market economies to come together, no excuses, no holding back, no arguments – to take full responsibility for restoring confidence.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [00:38<00:06,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' would risk overreacting, imposing unnecessary, wrongly directed, over-draconian conditions which would only harm the prospects for investment in our economies and jobs for the future.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:41<00:04,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' us see how we can set about it.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [00:43<00:02,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ask for your support to ensure that we have confident, well-informed consumers for electronic communications, who are also secure and know that their personal data is protected.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:46<00:00,  2.31s/it]\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.706246136687696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]], 'forced_eos_token_id': 0}\n"
     ]
    }
   ],
   "source": [
    "# collate function for tensor and padding before dataloader\n",
    "def collate_fn(batch):\n",
    "    input_features = [item['input_features'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    english_text = [item['english_text'] for item in batch]\n",
    "    \n",
    "    # Convert all inputs to tensors if they are not already\n",
    "    input_features = [torch.tensor(f) if not isinstance(f, torch.Tensor) else f for f in input_features]\n",
    "    labels = [torch.tensor(l) if not isinstance(l, torch.Tensor) else l for l in labels]\n",
    "    english_text = [torch.tensor(l) if not isinstance(l, torch.Tensor) else l for l in english_text]\n",
    "    \n",
    "    \n",
    "    # pad sequences\n",
    "    input_features = pad_sequence(input_features, batch_first=True)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=mt_tokenizer.pad_token_id)\n",
    "    english_text = pad_sequence(english_text, batch_first=True)\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'input_features': input_features,\n",
    "        'labels': labels, \n",
    "        'english_text': english_text\n",
    "    }\n",
    "\n",
    "# create data loader using collate function\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "print(dataloader)\n",
    "# optimizers\n",
    "whisper_optimizer = AdamW(whisper_model.parameters(), lr=5e-5)\n",
    "mt_optimizer = AdamW(mt_model.parameters(), lr=5e-5)\n",
    "\n",
    "# training loop\n",
    "num_epochs = 3\n",
    "output_dir = \"./fine_tuned_models\"\n",
    "\n",
    "        \n",
    "for epoch in range(3):\n",
    "    whisper_model.train()\n",
    "    mt_model.train()\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    for batch in tqdm(dataloader):\n",
    "        \n",
    "    \n",
    "        input_features = (batch[\"input_features\"])\n",
    "        transcription = batch[\"english_text\"]\n",
    "        target_ids = batch[\"labels\"]\n",
    "    \n",
    "        # forward pass through Whisper model with english transcription labels\n",
    "        whisper_outputs = whisper_model(input_features, labels=transcription)\n",
    "        \n",
    "        predicted_ids = whisper_outputs.logits.argmax(dim=-1)\n",
    "        \n",
    "        # decode the predicted ids to text\n",
    "        predicted_texts = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "        print(predicted_texts)\n",
    "\n",
    "        # tokenize the decoded text for the translation model\n",
    "        translation_inputs = mt_tokenizer(predicted_texts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "        # forward pass through Translation model with Whisper's transcription as input\n",
    "        translation_outputs = mt_model(input_ids=translation_inputs, labels=target_ids)\n",
    "\n",
    "        # combine the losses for end-to-end training\n",
    "        # combined_loss = whisper_outputs.loss + translation_outputs.loss\n",
    "        # ombined_loss.backward()\n",
    "       \n",
    "        whisper_outputs.loss.backward()\n",
    "        translation_outputs.loss.backward()\n",
    "        \n",
    "        whisper_optimizer.step()\n",
    "        mt_optimizer.step()\n",
    "        \n",
    "        whisper_optimizer.zero_grad()\n",
    "        mt_optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += whisper_outputs.loss.item() + translation_outputs.loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {avg_loss}\")\n",
    "\n",
    "    # save the models\n",
    "    whisper_model.save_pretrained(f\"./fine_tuned_models/whisper_epoch_{epoch + 1}\")\n",
    "    whisper_processor.save_pretrained(f\"./fine_tuned_models/whisper_epoch_{epoch + 1}\")\n",
    "    mt_model.save_pretrained(f\"./fine_tuned_models/mt_epoch_{epoch + 1}\")\n",
    "    mt_tokenizer.save_pretrained(f\"./fine_tuned_models/mt_epoch_{epoch + 1}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since tj-solergibert/Europarl-ST couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/jacksonbrouwer/.cache/huggingface/datasets/tj-solergibert___europarl-st/default/0.0.0/9635c1dc32c9a45e70e4373cf82e0c2e8fbe12c3 (last modified on Wed Aug 14 20:26:41 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1214\n",
      "Dataset({\n",
      "    features: ['original_speech', 'original_language', 'audio_path', 'segment_start', 'segment_end', 'transcriptions'],\n",
      "    num_rows: 1214\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/10 [00:00<?, ? examples/s]/var/folders/ds/zb_p84s57dl5dvdn322z_sd80000gn/T/ipykernel_4686/2065403734.py:13: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(audio_path, sr=16000, offset=segment_start, duration=segment_end-segment_start)\n",
      "/usr/local/lib/python3.12/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 14.74 examples/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /Helsinki-NLP/opus-mt-en-fr/resolve/main/model.safetensors (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x144106fc0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 9aa9d87c-ba65-48f3-9884-ac863c24f870)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/urllib3/connection.py:196\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py:964\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    963\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 964\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    965\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/urllib3/connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/urllib3/connection.py:615\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 615\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/urllib3/connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x144106fc0>: Failed to resolve 'huggingface.co' ([Errno 8] nodename nor servname provided, or not known)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Helsinki-NLP/opus-mt-en-fr/resolve/main/model.safetensors (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x144106fc0>: Failed to resolve 'huggingface.co' ([Errno 8] nodename nor servname provided, or not known)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m whisper_model \u001b[38;5;241m=\u001b[39m WhisperForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/whisper-tiny\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m mt_tokenizer \u001b[38;5;241m=\u001b[39m MarianTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-en-fr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m mt_model \u001b[38;5;241m=\u001b[39m \u001b[43mMarianMTModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHelsinki-NLP/opus-mt-en-fr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Device configuration\u001b[39;00m\n\u001b[1;32m     29\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/transformers/modeling_utils.py:3494\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3475\u001b[0m         has_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3476\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m: revision,\n\u001b[1;32m   3477\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxies\u001b[39m\u001b[38;5;124m\"\u001b[39m: proxies,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3480\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_files_only\u001b[39m\u001b[38;5;124m\"\u001b[39m: local_files_only,\n\u001b[1;32m   3481\u001b[0m         }\n\u001b[1;32m   3482\u001b[0m         cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3483\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   3484\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3492\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs,\n\u001b[1;32m   3493\u001b[0m         }\n\u001b[0;32m-> 3494\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mhas_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_weights_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhas_file_kwargs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   3495\u001b[0m             Thread(\n\u001b[1;32m   3496\u001b[0m                 target\u001b[38;5;241m=\u001b[39mauto_conversion,\n\u001b[1;32m   3497\u001b[0m                 args\u001b[38;5;241m=\u001b[39m(pretrained_model_name_or_path,),\n\u001b[1;32m   3498\u001b[0m                 kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore_errors_during_conversion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs},\n\u001b[1;32m   3499\u001b[0m                 name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThread-autoconversion\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3500\u001b[0m             )\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   3501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3502\u001b[0m     \u001b[38;5;66;03m# Otherwise, no PyTorch file was found, maybe there is a TF or Flax model file.\u001b[39;00m\n\u001b[1;32m   3503\u001b[0m     \u001b[38;5;66;03m# We try those to give a helpful error message.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/transformers/utils/hub.py:655\u001b[0m, in \u001b[0;36mhas_file\u001b[0;34m(path_or_repo, filename, revision, proxies, token, local_files_only, cache_dir, repo_type, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# Check if the file exists\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_hub_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_hf_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_user_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OfflineModeIsEnabled:\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m has_file_in_cache\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/requests/sessions.py:624\u001b[0m, in \u001b[0;36mSession.head\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a HEAD request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    623\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 624\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:66\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     68\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /Helsinki-NLP/opus-mt-en-fr/resolve/main/model.safetensors (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x144106fc0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 9aa9d87c-ba65-48f3-9884-ac863c24f870)')"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"tj-solergibert/Europarl-ST\", split=\"test\")\n",
    "test_dataset = test_dataset.filter(lambda example: example['original_language'] == 'en' and 'fr' in example['transcriptions'].keys() and example[\"transcriptions\"]['fr'] is not None)\n",
    "print(len(test_dataset))\n",
    "print(test_dataset)\n",
    "test_dataset = test_dataset.select(range(10))\n",
    "test_dataset = test_dataset.map(preprocess, remove_columns=[\"audio_path\", \"original_speech\", \"original_language\", \"segment_start\", \"segment_end\", \"transcriptions\"])\n",
    "\n",
    "# Create DataLoader with custom collate_fn\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Paths to the saved models\n",
    "whisper_model_path = \"./fine_tuned_models/whisper_epoch_3\"  # Update with your final epoch\n",
    "mt_model_path = \"./fine_tuned_models/mt_epoch_3\"  # Update with your final epoch\n",
    "\n",
    "# Load the models\n",
    "# whisper_processor = WhisperProcessor.from_pretrained(whisper_model_path)\n",
    "# whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_path)\n",
    "# mt_tokenizer = MarianTokenizer.from_pretrained(mt_model_path)\n",
    "# mt_model = MarianMTModel.from_pretrained(mt_model_path)\n",
    "\n",
    "whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "mt_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "mt_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "whisper_model.to(device)\n",
    "mt_model.to(device)\n",
    "\n",
    "whisper_model.eval()\n",
    "mt_model.eval()\n",
    "    \n",
    "epoch_loss = 0.0\n",
    "for batch in tqdm(test_dataloader):\n",
    "    input_features = batch[\"input_features\"].to(device)\n",
    "\n",
    "    original_speech = whisper_processor.batch_decode(batch[\"english_text\"].to(device), skip_special_tokens=True)[0]\n",
    "    print(\"Original Speech\", original_speech)\n",
    "    # Generate transcription using Whisper model\n",
    "    with torch.no_grad():\n",
    "        whisper_outputs = whisper_model.generate(\n",
    "            input_features,\n",
    "            num_beams=5, \n",
    "            repetition_penalty=1.2, \n",
    "            no_repeat_ngram_size=2, \n",
    "            temperature=0.7,  \n",
    "            top_k=50,  \n",
    "            top_p=0.95  \n",
    "        )\n",
    "        \n",
    "        transcription = whisper_processor.batch_decode(whisper_outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(\"Generated Transcription:\", transcription)\n",
    "\n",
    "    # Translate the transcription using the translation model\n",
    "    tokenized_transcription = mt_tokenizer(transcription, return_tensors=\"pt\", padding=\"longest\", truncation=True)\n",
    "    tokenized_transcription = tokenized_transcription.input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        translated_tokens = mt_model.generate(tokenized_transcription)\n",
    "        translation = mt_tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"Translation to French:\", translation)\n",
    "\n",
    "avg_loss = epoch_loss / len(dataloader)\n",
    "print(f\"Loss: {avg_loss}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Speech Why have policymakers not learnt from previous crises, though stern warnings were sent?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:03<00:28,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Why haven't policy-making learned from previous crisis? Those turn-and-warnings were sent.\n",
      "Generated Translation: Pourquoi l'élaboration des politiques n'a-t-elle pas appris de la crise précédente?\n",
      "Reference Translation: Pourquoi les décideurs politiques n'ont-ils pas tiré les leçons des précédentes crises, en dépit des avertissements qui leur étaient adressés?\n",
      "BLEU Score: 2.2008038097333027\n",
      "\n",
      "Original Speech What about the flaws of the originate-and-distribute model, which has enhanced systemic risk?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:05<00:22,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  What about the flaws of the region at industry be model, which has enhanced systemic risk?\n",
      "Generated Translation: Qu'en est-il des défauts de la région dans l'industrie, qui ont accru le risque systémique?\n",
      "Reference Translation: Qu'en est-il des défaillances du modèle d '« octroi puis cession », qui a augmenté le risque systémique?\n",
      "BLEU Score: 19.343786993461148\n",
      "\n",
      "Original Speech Madam President, are only greed, euphoria and cheap money to be blamed for the whole mess?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:08<00:18,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  He's only greed, euphoria, and cheap money to be blamed for the whole mess.\n",
      "Generated Translation: Il n'est que cupidité, euphorie, et argent bon marché à blâmer pour tout le désordre.\n",
      "Reference Translation: Madame la Présidente, l'avidité, l'euphorie et l'argent bon marché sont-ils les seuls à blâmer pour tout ce désordre?\n",
      "BLEU Score: 15.378249972287636\n",
      "\n",
      "Original Speech Just remember what Lamfalussy, Gramlich, Volcker and Buffett said years ago.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:10<00:16,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  just remember what Lumpfaloo's e-gramming forker. Buff had said years ago.\n",
      "Generated Translation: Souviens-toi de ce que Lumpfaloo a dit il y a des années.\n",
      "Reference Translation: Souvenez-vous de ce que disaient Lamfalussy, Gramlich, Volcker et Buffett il y a quelques années.\n",
      "BLEU Score: 14.957166830762677\n",
      "\n",
      "Original Speech What about conflicts of interest?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:12<00:10,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  about conflicts of interest.\n",
      "Generated Translation: sur les conflits d'intérêts.\n",
      "Reference Translation: Et les conflits d'intérêts?\n",
      "BLEU Score: 39.76353643835254\n",
      "\n",
      "Original Speech What about skewed pay schemes with a lack of ethics, which have stimulated reckless risk-taking?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:15<00:09,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  What about skewed paste schemes with a lack of ethics, which has stimulated recklessly staking?\n",
      "Generated Translation: Qu'en est-il des stratagèmes de pâte biaisés avec un manque d'éthique, qui a stimulé le jalonnement imprudent?\n",
      "Reference Translation: Qu'en est-il du système de rémunération, biaisé et totalement dépourvu d'éthique, qui a encouragé la prise de risques inconsidérés?\n",
      "BLEU Score: 13.830039740141478\n",
      "\n",
      "Original Speech What about investment-grade values assigned to trash?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:17<00:06,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  What about investment grade values assigned to trash?\n",
      "Generated Translation: Qu'en est-il des valeurs d'investissement assignées aux déchets?\n",
      "Reference Translation: Qu'en est-il des valeurs d'investissement qui ont jetées à la poubelle?\n",
      "BLEU Score: 33.47189874003769\n",
      "\n",
      "Original Speech The argument that regulation stifles financial innovation I find ludicrous.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:19<00:04,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  The argument that regulations type of financial innovation I find ludicrous.\n",
      "Generated Translation: L'argument selon lequel le type de réglementation d'innovation financière me paraît ridicule.\n",
      "Reference Translation: L'argument selon lequel la réglementation étoufferait l'innovation financière est tout simplement grotesque.\n",
      "BLEU Score: 13.67440667823257\n",
      "\n",
      "Original Speech What about banks engaging in casino-type transactions?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:21<00:02,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  What would banks engage in casino type transactions?\n",
      "Generated Translation: Qu'est-ce que les banques effectueraient dans des transactions de type casino?\n",
      "Reference Translation: Et les banques qui s'engagent dans des transactions de type casino?\n",
      "BLEU Score: 57.067457770559976\n",
      "\n",
      "Original Speech What about the ‘ shadow ’ banking sector, with its extreme leveraging and speculation?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:23<00:00,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  What about the shadow banking sector with extreme leveraging and speculation?\n",
      "Generated Translation: Qu'en est-il du secteur bancaire parallèle avec un effet de levier et des spéculations extrêmes?\n",
      "Reference Translation: Et le secteur bancaire « caché », qui mise à l'extrême sur l'effet de levier et la spéculation?\n",
      "BLEU Score: 9.330745616758765\n",
      "\n",
      "Average BLEU Score: 21.901809259032778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate BLEU score of the original model and the trained model \n",
    "import sacrebleu\n",
    "import jiwer\n",
    "\n",
    "bleu_scores = []\n",
    "wer_scores = []\n",
    "\n",
    "for batch in tqdm(test_dataloader):\n",
    "    input_features = batch[\"input_features\"].to(device)\n",
    "    target_ids = batch[\"labels\"].to(device)\n",
    "\n",
    "    original_speech = whisper_processor.batch_decode(batch[\"english_text\"].to(device), skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(\"Original Speech\", original_speech)\n",
    "    # Generate transcription using Whisper model\n",
    "    with torch.no_grad():\n",
    "        whisper_outputs = whisper_model.generate(\n",
    "            input_features,\n",
    "            num_beams=5, \n",
    "            repetition_penalty=1.2, \n",
    "            no_repeat_ngram_size=2, \n",
    "            temperature=0.7,  \n",
    "            top_k=50,  \n",
    "            top_p=0.95  \n",
    "        )\n",
    "        \n",
    "        transcription = whisper_processor.batch_decode(whisper_outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    # print(\"Generated Transcription:\", transcription)\n",
    "\n",
    "    # Translate the transcription using the translation model\n",
    "    tokenized_transcription = mt_tokenizer(transcription, return_tensors=\"pt\", padding=\"longest\", truncation=True)\n",
    "    tokenized_transcription = tokenized_transcription.input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        translated_tokens = mt_model.generate(tokenized_transcription)\n",
    "        translation = mt_tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    # print(\"Translation to French:\", translation)\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    reference = mt_tokenizer.decode(target_ids[0], skip_special_tokens=True)\n",
    "    hypothesis = translation \n",
    "    bleu = sacrebleu.corpus_bleu([hypothesis], [[reference]])\n",
    "    bleu_score = bleu.score\n",
    "    bleu_scores.append(bleu_score)\n",
    "    wer = jiwer.wer(reference, hypothesis)\n",
    "    wer_scores.append(wer)\n",
    "\n",
    "    print(f\"Transcription: {transcription}\")\n",
    "    print(f\"Generated Translation: {translation}\")\n",
    "    print(f\"Reference Translation: {reference}\")\n",
    "    print(f\"BLEU Score: {bleu_score}\\n\")\n",
    "    print(f\"WER: {wer}\\n\")\n",
    "\n",
    "avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "avg_wer_score = sum(wer_scores) / len(wer_scores)\n",
    "\n",
    "print(f\"Average BLEU Score: {avg_bleu_score}\")\n",
    "print(f\"Average WER: {avg_wer_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
